

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Evaluation &mdash; CFRL 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=2709fde1"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Custom Preprocessors" href="../customizations/custom_preprocessors.html" />
    <link rel="prev" title="FQE" href="fqe.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            CFRL
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../introduction/getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/computing_times.html">Computing Times</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/faq.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inputs and Outputs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../inputs_and_outputs/data_requirements.html">Data Requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../inputs_and_outputs/trajectory_arrays.html">Trajectory Arrays</a></li>
<li class="toctree-l1"><a class="reference internal" href="../inputs_and_outputs/tabular_trajectory_data.html">Tabular Trajectory Data</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/example_workflows.html">Example Workflows</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/common_issues.html">Common Issues</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Interface</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Interface</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="reader.html">Reader</a></li>
<li class="toctree-l2"><a class="reference internal" href="preprocessor.html">Preprocessor</a></li>
<li class="toctree-l2"><a class="reference internal" href="environment.html">Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="agents.html">Agents</a></li>
<li class="toctree-l2"><a class="reference internal" href="fqe.html">FQE</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#cfrl.evaluation.evaluate_fairness_through_model"><code class="docutils literal notranslate"><span class="pre">evaluate_fairness_through_model()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#cfrl.evaluation.evaluate_fairness_through_simulation"><code class="docutils literal notranslate"><span class="pre">evaluate_fairness_through_simulation()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#cfrl.evaluation.evaluate_reward_through_fqe"><code class="docutils literal notranslate"><span class="pre">evaluate_reward_through_fqe()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#cfrl.evaluation.evaluate_reward_through_simulation"><code class="docutils literal notranslate"><span class="pre">evaluate_reward_through_simulation()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#cfrl.evaluation.f_ua_default"><code class="docutils literal notranslate"><span class="pre">f_ua_default()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#cfrl.evaluation.f_ur_default"><code class="docutils literal notranslate"><span class="pre">f_ur_default()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#cfrl.evaluation.f_ux_default"><code class="docutils literal notranslate"><span class="pre">f_ux_default()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Customizations</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../customizations/custom_preprocessors.html">Custom Preprocessors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../customizations/custom_agents.html">Custom Agents</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About CFRL</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../about_cfrl/the_cfrl_team.html">The CFRL Team</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">CFRL</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Interface</a></li>
      <li class="breadcrumb-item active">Evaluation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/interface/evaluation.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="evaluation">
<h1>Evaluation<a class="headerlink" href="#evaluation" title="Link to this heading"></a></h1>
<p>This module implements functions used for evaluating the value and fairness of policies.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cfrl</span> <span class="kn">import</span> <span class="n">evaluation</span>
</pre></div>
</div>
<dl class="py function" id="module-cfrl.evaluation">
<dt class="sig sig-object py" id="cfrl.evaluation.evaluate_fairness_through_model">
<span class="sig-prename descclassname"><span class="pre">cfrl.evaluation.</span></span><span class="sig-name descname"><span class="pre">evaluate_fairness_through_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">env:</span> <span class="pre">~cfrl.environment.SimulatedEnvironment,</span> <span class="pre">zs:</span> <span class="pre">list</span> <span class="pre">|</span> <span class="pre">~numpy.ndarray,</span> <span class="pre">states:</span> <span class="pre">list</span> <span class="pre">|</span> <span class="pre">~numpy.ndarray,</span> <span class="pre">actions:</span> <span class="pre">list</span> <span class="pre">|</span> <span class="pre">~numpy.ndarray,</span> <span class="pre">policy:</span> <span class="pre">~cfrl.agents.Agent,</span> <span class="pre">f_ua:</span> <span class="pre">~typing.Callable[[int],</span> <span class="pre">int]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">f_ua_default&gt;,</span> <span class="pre">seed:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">integer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">floating</span></span></span><a class="headerlink" href="#cfrl.evaluation.evaluate_fairness_through_model" title="Link to this definition"></a></dt>
<dd><p>Estimate the counterfactual fairness metric of a policy from an offline trajectory.</p>
<p>The function first estimates a set of counterfactual trajectories from the offline trajectory 
using <code class="code docutils literal notranslate"><span class="pre">estimate_counterfactual_trajectories_from_data()</span></code> in the <code class="code docutils literal notranslate"><span class="pre">environment</span></code> module. 
Then it computes a counterfactual fairness metric using the following formula given in Wang et al. 
(2025):</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\max_{z', z \in eval(Z)} \frac{1}{NT} \sum_{i=1}^{N} \sum_{t=1}^{T} 
\mathbb{I} \left( A_t^{Z \leftarrow z'}\left(\bar{U}_t(h_{it})\right) 
\neq A_t^{Z \leftarrow z}\left(\bar{U}_t(h_{it})\right) \right).\]</div>
</div></blockquote>
<p>where <span class="math notranslate nohighlight">\(eval(Z)\)</span> is the set of sensitive attribute values passed in by <cite>z_eval_levels</cite>, 
<span class="math notranslate nohighlight">\(A_t^{Z \leftarrow z'}\left(\bar{U}_t(h_{it})\right)\)</span> is the action taken in the 
counterfactual trajectory under <span class="math notranslate nohighlight">\(Z=z'\)</span>, and 
<span class="math notranslate nohighlight">\(A_t^{Z \leftarrow z}\left(\bar{U}_t(h_{it})\right)\)</span> is the action taken under the 
counterfactual trajectory under <span class="math notranslate nohighlight">\(Z=z\)</span>. This metric is bounded between 0 and 1, with 0 
representing perfect fairness and 1 indicating complete unfairness.</p>
<dl>
<dt>References: </dt><dd><aside class="footnote-list brackets">
<aside class="footnote brackets" id="id1" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<p>Wang, J., Shi, C., Piette, J.D., Loftus, J.R., Zeng, D. and Wu, Z. (2025). 
Counterfactually Fair Reinforcement Learning via Sequential Data 
Preprocessing. arXiv preprint arXiv:2501.06366.</p>
</aside>
</aside>
</dd>
<dt>Args: </dt><dd><dl class="simple">
<dt>env (SimulatedEnvironment): </dt><dd><p>An environment that simulates the transition dynamics of the 
MDP underlying <code class="code docutils literal notranslate"><span class="pre">zs</span></code>, <code class="code docutils literal notranslate"><span class="pre">states</span></code>, <code class="code docutils literal notranslate"><span class="pre">actions</span></code>, and <code class="code docutils literal notranslate"><span class="pre">rewards</span></code>.</p>
</dd>
<dt>zs (list or np.ndarray): </dt><dd><p>The observed sensitive attributes of each individual in the 
offline trajectory. It should be a list or array following the Sensitive Attributes 
Format.</p>
</dd>
<dt>states (list or np.ndarray): </dt><dd><p>The state trajectory. It should be a list or array following 
the Full-trajectory States Format.</p>
</dd>
<dt>actions (list or np.ndarray): </dt><dd><p>The action trajectory. It should be a list or array following 
the Full-trajectory Actions Format.</p>
</dd>
<dt>policy (Agent): </dt><dd><p>The policy whose fairness is to be evaluated.</p>
</dd>
<dt>f_ua (Callable, optional): </dt><dd><p>A rule to generate exogenous variables for each individual’s 
actions. It should be a function whose argument list, argument names, and return 
type exactly match those of <code class="code docutils literal notranslate"><span class="pre">f_ua_default</span></code>.</p>
</dd>
<dt>seed (int, optional): </dt><dd><p>The seed used to estimate the counterfactual trajectories.</p>
</dd>
</dl>
</dd>
<dt>Returns: </dt><dd><dl class="simple">
<dt>cf_metric (np.integer or np.floating): </dt><dd><p>The counterfactual fairness metric of the policy.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="cfrl.evaluation.evaluate_fairness_through_simulation">
<span class="sig-prename descclassname"><span class="pre">cfrl.evaluation.</span></span><span class="sig-name descname"><span class="pre">evaluate_fairness_through_simulation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">env:</span> <span class="pre">~cfrl.environment.SyntheticEnvironment,</span> <span class="pre">z_eval_levels:</span> <span class="pre">list</span> <span class="pre">|</span> <span class="pre">~numpy.ndarray,</span> <span class="pre">state_dim:</span> <span class="pre">int,</span> <span class="pre">N:</span> <span class="pre">int,</span> <span class="pre">T:</span> <span class="pre">int,</span> <span class="pre">policy:</span> <span class="pre">~cfrl.agents.Agent,</span> <span class="pre">f_ux:</span> <span class="pre">~typing.Callable[[int,</span> <span class="pre">int],</span> <span class="pre">int]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">f_ux_default&gt;,</span> <span class="pre">f_ua:</span> <span class="pre">~typing.Callable[[int],</span> <span class="pre">int]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">f_ua_default&gt;,</span> <span class="pre">f_ur:</span> <span class="pre">~typing.Callable[[int],</span> <span class="pre">int]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">f_ur_default&gt;,</span> <span class="pre">z_probs:</span> <span class="pre">list</span> <span class="pre">|</span> <span class="pre">~numpy.ndarray</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">seed:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">integer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">floating</span></span></span><a class="headerlink" href="#cfrl.evaluation.evaluate_fairness_through_simulation" title="Link to this definition"></a></dt>
<dd><p>Estimate the counterfactual fairness metric of a policy using simulation in a synthetic environment.</p>
<p>The function first simulates a set of counterfactual trajectories with a pre-specified length 
using <code class="code docutils literal notranslate"><span class="pre">sample_counterfactual_trajectories()</span></code> in the <cite>environment</cite> module. Then it computes 
a counterfactual fairness metric using the following formula given in Wang et al. (2025):</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\max_{z', z \in eval(Z)} \frac{1}{NT} \sum_{i=1}^{N} \sum_{t=1}^{T} 
\mathbb{I} \left( A_t^{Z \leftarrow z'}\left(\bar{U}_t(h_{it})\right) 
\neq A_t^{Z \leftarrow z}\left(\bar{U}_t(h_{it})\right) \right).\]</div>
</div></blockquote>
<p>where <span class="math notranslate nohighlight">\(eval(Z)\)</span> is the set of sensitive attribute values passed in by <cite>z_eval_levels</cite>, 
<span class="math notranslate nohighlight">\(A_t^{Z \leftarrow z'}\left(\bar{U}_t(h_{it})\right)\)</span> is the action taken in the 
counterfactual trajectory under <span class="math notranslate nohighlight">\(Z=z'\)</span>, and 
<span class="math notranslate nohighlight">\(A_t^{Z \leftarrow z}\left(\bar{U}_t(h_{it})\right)\)</span> is the action taken under the 
counterfactual trajectory under <span class="math notranslate nohighlight">\(Z=z\)</span>. This metric is bounded between 0 and 1, with 0 
representing perfect fairness and 1 indicating complete unfairness.</p>
<dl>
<dt>References: </dt><dd><aside class="footnote-list brackets">
<aside class="footnote brackets" id="id2" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<p>Wang, J., Shi, C., Piette, J.D., Loftus, J.R., Zeng, D. and Wu, Z. (2025). 
Counterfactually Fair Reinforcement Learning via Sequential Data 
Preprocessing. arXiv preprint arXiv:2501.06366.</p>
</aside>
</aside>
</dd>
<dt>Args: </dt><dd><dl class="simple">
<dt>env (SyntheticEnvironment): </dt><dd><p>The synthetic environment in which the simulation is run.</p>
</dd>
<dt>z_eval_levels (list or np.ndarray): </dt><dd><p>The values of sensitive attributes for which 
counterfactual trajectories are generated in the simulation. 
The observed sensitive attributes of the individuals in the simulation will also be 
sampled from this set.</p>
</dd>
<dt>state_dim (int): </dt><dd><p>The number of components in the state vector.</p>
</dd>
<dt>N (int): </dt><dd><p>The total number of individuals in the counterfactual trajectories sampled during the 
simulation.</p>
</dd>
<dt>T (int): </dt><dd><p>The total number of transitions in the counterfactual trajectories sampled during the 
simulation.</p>
</dd>
<dt>policy (Agent): </dt><dd><p>The policy whose fairness is to be evaluated.</p>
</dd>
<dt>f_ux (Callable, optional): </dt><dd><p>A rule to generate exogenous variables for each individual’s 
states. It should be a function whose argument list, argument names, and return 
type exactly match those of <code class="code docutils literal notranslate"><span class="pre">f_ux_default</span></code>.</p>
</dd>
<dt>f_ua (Callable, optional): </dt><dd><p>A rule to generate exogenous variables for each individual’s 
actions. It should be a function whose argument list, argument names, and return 
type exactly match those of <code class="code docutils literal notranslate"><span class="pre">f_ua_default</span></code>.</p>
</dd>
<dt>f_ur (Callable, optional): </dt><dd><p>A rule to generate exogenous variables for each individual’s 
rewards. It should be a function whose argument list, argument names, and return 
type exactly match those of <code class="code docutils literal notranslate"><span class="pre">f_ur_default</span></code>.</p>
</dd>
<dt>z_probs (list or np.ndarray, optional): </dt><dd><p>The probability of an individual taking each of 
the values in <code class="code docutils literal notranslate"><span class="pre">z_eval_levels</span></code> as the observed sensitive attribute. When set to 
<code class="code docutils literal notranslate"><span class="pre">None</span></code>, a uniform distribution will be used.</p>
</dd>
<dt>seed (int, optional): </dt><dd><p>The random seed used to run the simulation.</p>
</dd>
</dl>
</dd>
<dt>Returns: </dt><dd><dl class="simple">
<dt>cf_metric (np.integer or np.floating): </dt><dd><p>The counterfactual fairness metric of the policy.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="cfrl.evaluation.evaluate_reward_through_fqe">
<span class="sig-prename descclassname"><span class="pre">cfrl.evaluation.</span></span><span class="sig-name descname"><span class="pre">evaluate_reward_through_fqe</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">zs:</span> <span class="pre">list</span> <span class="pre">|</span> <span class="pre">~numpy.ndarray,</span> <span class="pre">states:</span> <span class="pre">list</span> <span class="pre">|</span> <span class="pre">~numpy.ndarray,</span> <span class="pre">actions:</span> <span class="pre">list</span> <span class="pre">|</span> <span class="pre">~numpy.ndarray,</span> <span class="pre">rewards:</span> <span class="pre">list</span> <span class="pre">|</span> <span class="pre">~numpy.ndarray,</span> <span class="pre">policy:</span> <span class="pre">~cfrl.agents.Agent,</span> <span class="pre">model_type:</span> <span class="pre">~typing.Literal['lm',</span> <span class="pre">'nn'],</span> <span class="pre">f_ua:</span> <span class="pre">~typing.Callable[[int],</span> <span class="pre">int]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">f_ua_default&gt;,</span> <span class="pre">hidden_dims:</span> <span class="pre">list[int]</span> <span class="pre">=</span> <span class="pre">[32],</span> <span class="pre">learning_rate:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.1,</span> <span class="pre">epochs:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">500,</span> <span class="pre">gamma:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.9,</span> <span class="pre">max_iter:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">200,</span> <span class="pre">seed:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1,</span> <span class="pre">is_loss_monitored:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False,</span> <span class="pre">is_early_stopping_nn:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False,</span> <span class="pre">test_size_nn:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.2,</span> <span class="pre">loss_monitoring_patience:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">10,</span> <span class="pre">loss_monitoring_min_delta:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.005,</span> <span class="pre">early_stopping_patience_nn:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">10,</span> <span class="pre">early_stopping_min_delta_nn:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.005,</span> <span class="pre">is_q_monitored:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True,</span> <span class="pre">is_early_stopping_q:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True,</span> <span class="pre">q_monitoring_patience:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">5,</span> <span class="pre">q_monitoring_min_delta:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.005,</span> <span class="pre">early_stopping_patience_q:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">5,</span> <span class="pre">early_stopping_min_delta_q:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.005</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">integer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">floating</span></span></span><a class="headerlink" href="#cfrl.evaluation.evaluate_reward_through_fqe" title="Link to this definition"></a></dt>
<dd><p>Estimate the value of a policy using fitted Q evaluation (FQE).</p>
<p>The function takes in a offline trajectory and the policy of interest, which are then used by 
a FQE algorithm to evaluate the value of the policy.</p>
<dl class="simple">
<dt>Args: </dt><dd><dl class="simple">
<dt>zs (list or np.ndarray): </dt><dd><p>The observed sensitive attributes of each individual in the 
offline trajectory. It should be a list or array following the Sensitive Attributes 
Format.</p>
</dd>
<dt>states (list or np.ndarray): </dt><dd><p>The state trajectory. It should be a list or array following 
the Full-trajectory States Format.</p>
</dd>
<dt>actions (list or np.ndarray): </dt><dd><p>The action trajectory. It should be a list or array following 
the Full-trajectory Actions Format.</p>
</dd>
<dt>rewards (list or np.ndarray): </dt><dd><p>The reward trajectory. It should be a list or array following 
the Full-trajectory Rewards Format.</p>
</dd>
<dt>policy (Agent): </dt><dd><p>The policy whose value is to be evaluated.</p>
</dd>
<dt>model_type (str): </dt><dd><p>The type of the model used for FQE. Can be “lm” (polynomial regression) or 
“nn” (neural network). <em>Currently, only ‘nn’ is supported.</em></p>
</dd>
<dt>f_ua (Callable, optional): </dt><dd><p>A rule to generate exogenous variables for each individual’s 
actions during training. It should be a function whose argument list, argument names, 
and return type exactly match those of <code class="code docutils literal notranslate"><span class="pre">f_ua_default</span></code>.</p>
</dd>
<dt>hidden_dims (list[int], optional): </dt><dd><p>The hidden dimensions of the neural network. This 
argument is not used if <code class="code docutils literal notranslate"><span class="pre">model_type=&quot;lm&quot;</span></code>.</p>
</dd>
<dt>learning_rate (int or float, optional): </dt><dd><p>The learning rate of the neural network. This 
argument is not used if <code class="code docutils literal notranslate"><span class="pre">model_type=&quot;lm&quot;</span></code>.</p>
</dd>
<dt>epochs (int, optional): </dt><dd><p>The number of training epochs for the neural network. This 
argument is not used if <code class="code docutils literal notranslate"><span class="pre">model_type=&quot;lm&quot;</span></code>.</p>
</dd>
<dt>gamma (int or float, optional): </dt><dd><p>The discount factor for the cumulative discounted reward 
in the objective function.</p>
</dd>
<dt>max_iter (int, optional): </dt><dd><p>The number of iterations for learning the Q function.</p>
</dd>
<dt>seed (int, optional): </dt><dd><p>The random seed used for FQE.</p>
</dd>
<dt>is_loss_monitored (bool, optional):</dt><dd><p>When set to <code class="code docutils literal notranslate"><span class="pre">True</span></code>, will split the training data into a training set and a 
validation set, and will monitor the validation loss when training the neural network 
approximator of the Q function in each iteration. A warning 
will be raised if the percent absolute change in the validation loss is greater than <code class="code docutils literal notranslate"><span class="pre">loss_monitoring_min_delta</span></code> for at 
least one of the final <span class="math notranslate nohighlight">\(p\)</span> epochs during neural network training, where <span class="math notranslate nohighlight">\(p\)</span> is specified 
by the argument <code class="code docutils literal notranslate"><span class="pre">loss_monitoring_patience</span></code>. This argument is not used if <code class="code docutils literal notranslate"><span class="pre">model_type=&quot;lm&quot;</span></code>.</p>
</dd>
<dt>is_early_stopping_nn (bool, optional): </dt><dd><p>When set to <code class="code docutils literal notranslate"><span class="pre">True</span></code>, will split the training data into a training set and a 
validation set, and will enforce early stopping based on the validation loss 
when training the neural network approximator of the Q function in each iteration. That is, in each iteration, 
neural network training will stop early 
if the percent decrease in the validation loss is no greater than <code class="code docutils literal notranslate"><span class="pre">early_stopping_min_delta_nn</span></code> for <span class="math notranslate nohighlight">\(q\)</span> consecutive training 
epochs, where <span class="math notranslate nohighlight">\(q\)</span> is specified by the argument <code class="code docutils literal notranslate"><span class="pre">early_stopping_patience_nn</span></code>. This argument is not used if 
<code class="code docutils literal notranslate"><span class="pre">model_type=&quot;lm&quot;</span></code>.</p>
</dd>
<dt>test_size_nn (int or float, optional): </dt><dd><p>An <code class="code docutils literal notranslate"><span class="pre">int</span></code> or <code class="code docutils literal notranslate"><span class="pre">float</span></code> between 0 and 1 (inclusive) that 
specifies the proportion of the full training data that is used as the validation set for loss 
monitoring and early stopping. This argument is not used if <code class="code docutils literal notranslate"><span class="pre">model_type=&quot;lm&quot;</span></code> or 
both <code class="code docutils literal notranslate"><span class="pre">is_loss_monitored</span></code> and <code class="code docutils literal notranslate"><span class="pre">is_early_stopping</span></code> are <code class="code docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
<dt>loss_monitoring_patience (int, optional): </dt><dd><p>The number of consecutive epochs with barely-changing validation loss at the end of neural network training that is needed 
for loss monitoring to not raise warnings. This argument is not used if <code class="code docutils literal notranslate"><span class="pre">model_type=&quot;lm&quot;</span></code> 
or <code class="code docutils literal notranslate"><span class="pre">is_loss_monitored=False</span></code>.</p>
</dd>
<dt>loss_monitoring_min_delta (int for float, optional): </dt><dd><p>The maximum amount of percent absolute change in the validation loss for it to be considered 
barely-changing by the loss monitoring mechanism. This argument is 
not used if <code class="code docutils literal notranslate"><span class="pre">model_type=&quot;lm&quot;</span></code> or <code class="code docutils literal notranslate"><span class="pre">is_loss_monitored=False</span></code>.</p>
</dd>
<dt>early_stopping_patience_nn (int, optional): </dt><dd><p>The number of consecutive epochs with barely-decreasing validation loss during neural network training that is needed 
for early stopping to be triggered. This argument is not used if <code class="code docutils literal notranslate"><span class="pre">model_type=&quot;lm&quot;</span></code> 
or <code class="code docutils literal notranslate"><span class="pre">is_early_stopping_nn=False</span></code>.</p>
</dd>
<dt>early_stopping_min_delta_nn (int for float, optional): </dt><dd><p>The maximum amount of decrease in the validation loss for it to be considered 
barely-decreasing by the early stopping mechanism. This argument is 
not used if <code class="code docutils literal notranslate"><span class="pre">model_type=&quot;lm&quot;</span></code> or <code class="code docutils literal notranslate"><span class="pre">is_early_stopping_nn=False</span></code>.</p>
</dd>
<dt>is_q_monitored (bool, optional):</dt><dd><p>When set to <code class="code docutils literal notranslate"><span class="pre">True</span></code>, will monitor the Q values estimated by the neural network 
approximator of the Q function in each iteration at all the state-action pairs present in the training trajectory. A warning 
will be raised if the percent absolute change in some Q value is greater than <code class="code docutils literal notranslate"><span class="pre">q_monitoring_min_delta</span></code> for at 
least one of the final <span class="math notranslate nohighlight">\(r\)</span> iterations of model updates, where <span class="math notranslate nohighlight">\(r\)</span> is specified 
by the argument <code class="code docutils literal notranslate"><span class="pre">q_monitoring_patience</span></code>. This argument is not used if <code class="code docutils literal notranslate"><span class="pre">model_type=&quot;lm&quot;</span></code>.</p>
</dd>
<dt>is_early_stopping_q (bool, optional): </dt><dd><p>When set to <code class="code docutils literal notranslate"><span class="pre">True</span></code>, will monitor the Q values estimated by the neural network 
approximator of the Q function at all the state-action pairs present in the training trajectory, 
and will enforce early stopping based on the estimated Q values 
when training the approximated Q function. That is, 
FQE training will stop early 
if the percent absolute changes in all the predicted Q values are no greater than <code class="code docutils literal notranslate"><span class="pre">early_stopping_min_delta_q</span></code> for <span class="math notranslate nohighlight">\(s\)</span> consecutive 
iterations of model updates, where <span class="math notranslate nohighlight">\(s\)</span> is specified by the argument <code class="code docutils literal notranslate"><span class="pre">early_stopping_patience_q</span></code>. This argument is not used if 
<code class="code docutils literal notranslate"><span class="pre">model_type=&quot;lm&quot;</span></code>.</p>
</dd>
<dt>q_monitoring_patience (int, optional): </dt><dd><p>The number of consecutive iterations with barely-changing estimated Q values at the end of the iterative updates that is needed 
for Q value monitoring to not raise warnings. This argument is not used if <code class="code docutils literal notranslate"><span class="pre">model_type=&quot;lm&quot;</span></code> 
or <code class="code docutils literal notranslate"><span class="pre">is_q_monitored=False</span></code>.</p>
</dd>
<dt>q_monitoring_min_delta (int for float, optional): </dt><dd><p>The maximum amount of persent absolute change in the estimated Q values for them to be considered 
barely-changing by the Q value monitoring mechanism. This argument is 
not used if <code class="code docutils literal notranslate"><span class="pre">model_type=&quot;lm&quot;</span></code> or <code class="code docutils literal notranslate"><span class="pre">is_q_monitored=False</span></code>.</p>
</dd>
<dt>early_stopping_patience_q (int, optional): </dt><dd><p>The number of consecutive iterations with barely-changing estimated Q values that is needed 
for early stopping to be triggered. This argument is not used if <code class="code docutils literal notranslate"><span class="pre">model_type=&quot;lm&quot;</span></code> 
or <code class="code docutils literal notranslate"><span class="pre">is_early_stopping_q=False</span></code>.</p>
</dd>
<dt>early_stopping_min_delta_q (int for float, optional): </dt><dd><p>The maximum amount of percent absolute change in the estimated Q values for them to be considered 
barely-changing by the early stopping mechanism. This argument is 
not used if <code class="code docutils literal notranslate"><span class="pre">model_type=&quot;lm&quot;</span></code> or <code class="code docutils literal notranslate"><span class="pre">is_early_stopping_q=False</span></code>.</p>
</dd>
</dl>
</dd>
<dt>Returns: </dt><dd><dl class="simple">
<dt>discounted_cumulative_reward (np.integer or np.floating): </dt><dd><p>An estimation of the discounted 
cumulative reward achieved by the policy throughout the trajectory.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="cfrl.evaluation.evaluate_reward_through_simulation">
<span class="sig-prename descclassname"><span class="pre">cfrl.evaluation.</span></span><span class="sig-name descname"><span class="pre">evaluate_reward_through_simulation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">env:</span> <span class="pre">~cfrl.environment.SyntheticEnvironment,</span> <span class="pre">z_eval_levels:</span> <span class="pre">list</span> <span class="pre">|</span> <span class="pre">~numpy.ndarray,</span> <span class="pre">state_dim:</span> <span class="pre">int,</span> <span class="pre">N:</span> <span class="pre">int,</span> <span class="pre">T:</span> <span class="pre">int,</span> <span class="pre">policy:</span> <span class="pre">~cfrl.agents.Agent,</span> <span class="pre">f_ux:</span> <span class="pre">~typing.Callable[[int,</span> <span class="pre">int],</span> <span class="pre">~numpy.ndarray]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">f_ux_default&gt;,</span> <span class="pre">f_ua:</span> <span class="pre">~typing.Callable[[int],</span> <span class="pre">~numpy.ndarray]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">f_ua_default&gt;,</span> <span class="pre">f_ur:</span> <span class="pre">~typing.Callable[[int],</span> <span class="pre">~numpy.ndarray]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">f_ur_default&gt;,</span> <span class="pre">z_probs:</span> <span class="pre">list</span> <span class="pre">|</span> <span class="pre">~numpy.ndarray</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">gamma:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.9,</span> <span class="pre">seed:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">integer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">floating</span></span></span><a class="headerlink" href="#cfrl.evaluation.evaluate_reward_through_simulation" title="Link to this definition"></a></dt>
<dd><p>Estimate the value of a policy using simulation in a synthetic environment.</p>
<p>The function first simulates a trajectory of a pre-specified length <code class="code docutils literal notranslate"><span class="pre">T</span></code> using the policy. 
Then it computes the cumulative discounted rewards achieved throughout the trajectory.</p>
<p>Since the discounted rewards are added across all time steps, it should generally be higher 
if a larger value is specified for the argument <code class="code docutils literal notranslate"><span class="pre">T</span></code>.</p>
<dl class="simple">
<dt>Args: </dt><dd><dl class="simple">
<dt>env (SyntheticEnvironment): </dt><dd><p>The synthetic environment in which the simulation is run.</p>
</dd>
<dt>z_eval_levels (list or np.ndarray): </dt><dd><p>The values of sensitive attributes used in the simulation. 
The observed sensitive attributes of the individuals in the simulation will be sampled 
from this set.</p>
</dd>
<dt>state_dim (int): </dt><dd><p>The number of components in the state vector.</p>
</dd>
<dt>N (int): </dt><dd><p>The total number of individuals in the trajectory sampled during the simulation.</p>
</dd>
<dt>T (int): </dt><dd><p>The total number of transitions in the trajectory sampled during the simulation.</p>
</dd>
<dt>policy (Agent): </dt><dd><p>The policy whose value is to be evaluated.</p>
</dd>
<dt>f_ux (Callable, optional): </dt><dd><p>A rule to generate exogenous variables for each individual’s 
states. It should be a function whose argument list, argument names, and return 
type exactly match those of <code class="code docutils literal notranslate"><span class="pre">f_ux_default</span></code>.</p>
</dd>
<dt>f_ua (Callable, optional): </dt><dd><p>A rule to generate exogenous variables for each individual’s 
actions. It should be a function whose argument list, argument names, and return 
type exactly match those of <code class="code docutils literal notranslate"><span class="pre">f_ua_default</span></code>.</p>
</dd>
<dt>f_ur (Callable, optional): </dt><dd><p>A rule to generate exogenous variables for each individual’s 
rewards. It should be a function whose argument list, argument names, and return 
type exactly match those of <code class="code docutils literal notranslate"><span class="pre">f_ur_default</span></code>.</p>
</dd>
<dt>z_probs (list or np.ndarray, optional): </dt><dd><p>The probability of an individual taking each of 
the values in <code class="code docutils literal notranslate"><span class="pre">z_eval_levels</span></code>. When set to <code class="code docutils literal notranslate"><span class="pre">None</span></code>, a uniform distribution 
will be used.</p>
</dd>
<dt>gamma (int or float, optional): </dt><dd><p>The discount factor used for calculating the discounted 
cumulative rewards.</p>
</dd>
<dt>seed (int, optional): </dt><dd><p>The random seed used to run the simulation.</p>
</dd>
</dl>
</dd>
<dt>Returns: </dt><dd><dl class="simple">
<dt>discounted_cumulative_reward (np.integer or np.floating): </dt><dd><p>An estimation of the discounted 
cumulative reward achieved by the policy throughout the trajectory.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="cfrl.evaluation.f_ua_default">
<span class="sig-prename descclassname"><span class="pre">cfrl.evaluation.</span></span><span class="sig-name descname"><span class="pre">f_ua_default</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">N</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span></span></span><a class="headerlink" href="#cfrl.evaluation.f_ua_default" title="Link to this definition"></a></dt>
<dd><p>Generate exogenous variables for the actions from a uniform distribution between 0 and 1.</p>
<dl class="simple">
<dt>Args: </dt><dd><dl class="simple">
<dt>N (int): </dt><dd><p>The total number of individuals for whom the exogenous variables will 
be generated.</p>
</dd>
</dl>
</dd>
<dt>Returns: </dt><dd><dl class="simple">
<dt>ua (np.ndarray): </dt><dd><p>The generated exogenous variables. It is a (N, 1) array 
where each entry is sampled from a uniform distribution between 0 and 1.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="cfrl.evaluation.f_ur_default">
<span class="sig-prename descclassname"><span class="pre">cfrl.evaluation.</span></span><span class="sig-name descname"><span class="pre">f_ur_default</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">N</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span></span></span><a class="headerlink" href="#cfrl.evaluation.f_ur_default" title="Link to this definition"></a></dt>
<dd><p>Generate exogenous variables for the rewards from a standard normal distribution.</p>
<dl class="simple">
<dt>Args: </dt><dd><dl class="simple">
<dt>N (int): </dt><dd><p>The total number of individuals for whom the exogenous variables will 
be generated.</p>
</dd>
</dl>
</dd>
<dt>Returns: </dt><dd><dl class="simple">
<dt>ur (np.ndarray): </dt><dd><p>The generated exogenous variables. It is a (N, 1) array 
where each entry is sampled from a standard normal distribution.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="cfrl.evaluation.f_ux_default">
<span class="sig-prename descclassname"><span class="pre">cfrl.evaluation.</span></span><span class="sig-name descname"><span class="pre">f_ux_default</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">N</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span></span></span><a class="headerlink" href="#cfrl.evaluation.f_ux_default" title="Link to this definition"></a></dt>
<dd><p>Generate exogenous variables for the states from a standard normal distribution.</p>
<dl class="simple">
<dt>Args: </dt><dd><dl class="simple">
<dt>N (int): </dt><dd><p>The total number of individuals for whom the exogenous variables will 
be generated.</p>
</dd>
<dt>state_dim (int): </dt><dd><p>The number of components in the state vector.</p>
</dd>
</dl>
</dd>
<dt>Returns: </dt><dd><dl class="simple">
<dt>ux (np.ndarray): </dt><dd><p>The generated exogenous variables. It is a (N, <code class="code docutils literal notranslate"><span class="pre">state_dim</span></code>) array 
where each entry is sampled from a standard normal distribution.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="fqe.html" class="btn btn-neutral float-left" title="FQE" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../customizations/custom_preprocessors.html" class="btn btn-neutral float-right" title="Custom Preprocessors" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, The CFRL Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>